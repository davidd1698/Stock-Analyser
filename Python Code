from datetime import datetime
import lxml
from lxml import html
import requests
import pandas as pd
import numpy as np
import pandas_datareader as web
import matplotlib.pyplot as plt

# Ask for the number of stocks the user wants to preselect
number = None
while number is None:
  try:
    number = int(input("Type the number of stocks you want to preselect: "))
  except ValueError:
    print ("You must type an integer, please try again.")

# date checking functions
def month_check(month):
    if month > 0 and month <= 12: ## If month is between 1 and 12, return True.
        return True
    else:
        return False

def day_check(month, day):
    days_in_month = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}
    if month > 0 and month <= 12 and 0 < day <= days_in_month[month]:
        return True
    else:
        return False

def year_check(year):
    if len(year) >= 1 and len(year) <= 4: ## Check if year has between 1 to 4 numbers and return True.
        return True
    else:
        return False

earliest = str("1990-01-01")

def start_date_check(start_date):
  if startdate > earliest:
    return True
  else: 
    return False

today = str("2021-04-30")

def end_date_check(start_date):
  if enddate < today:
    return True
  else: 
    return False

def start_before_end(startdate, enddate):
  if startdate < enddate:
    return True
  else: 
    return False

# ask start date and check if correct
startdate = None
while startdate is None:
  try: 
    startdate = str(input("Enter the start date in YYYY-MM-DD format: ")) 
    year,month,day = startdate.split("-") 
    month_validity = month_check(int(month)) 
    day_validity = day_check(int(month),int(day)) 
    year_validity = year_check(year)
    start_validity = start_date_check(startdate)

    if month_validity and day_validity and year_validity and start_validity:
      print("The date {0} is valid.".format(startdate))

    else:
      print("The date {0} is invalid.".format(startdate))
      startdate = None
      
  except ValueError:
    print('Your input was not valid. Please enter the date in YYYY-MM-DD format.')  
    startdate = None
     

#Ask for end date
enddate = None
while enddate is None:
  try: 
    enddate = str(input("Enter the end date in YYYY-MM-DD format: ")) 
    year,month,day = enddate.split("-") 
    month_validity = month_check(int(month)) 
    day_validity = day_check(int(month),int(day)) 
    year_validity = year_check(year)
    end_validity = end_date_check(enddate)
    start_end_validity = start_before_end(startdate, enddate)

    if month_validity and day_validity and year_validity and end_validity and start_end_validity : 
      print("The date {0} is valid.".format(enddate))

    else:
      print("The date {0} is invalid.".format(enddate))
      enddate = None

  except ValueError:
    print('Your input was not valid. Please enter the date in YYYY-MM-DD format.')
    enddate = None
  
    
### end date part
##############################################

# Ask the user to input the tickers
tickers = []
i = 1
while i <= number: 
  try: 
    tick = str(input(f'Enter a {i}th Ticker: ')) 
    tick = tick.upper()
    tickers.append(tick)
    i += 1 
  except ValueError:
    print("Error - you have to enter a string. Try again.") 

# Print the ticker list
print("the ticker list is:", tickers )

# Ask the user to input the risk free rate
riskfree = None
while riskfree is None:
  try:
    riskfree = float(input("Enter the risk free rate in decimals: "))
  except ValueError:
    print ("You must type a float")


# Getting historical stock prices and volume from Yahoo Finance
price_historical = web.get_data_yahoo(tickers,
                           start = startdate,
                           end = enddate)['Adj Close']
                           
volume_historical = web.get_data_yahoo(tickers,
                           start = startdate,
                           end = enddate)['Volume']

# Remove stocks that were not available during this time period and remove the tickers that do not exist
price_historical = price_historical.dropna(axis=1, how='all')

# Computing the log returns
log_ret = np.log(price_historical/price_historical.shift(1))

###################################################################################

# We should add here a part where we get additional info from yahoo finance for each preselected stock and we compute for each stock some performance and risk metrics. Based on this additional information, the user then selects the final sample of stocks he wants to put in his portfolio and the rest of the code does the 3 types of portfolio optimization.

# Set up the request headers that we're going to use, to simulate a request by the Chrome browser.
def get_page(url):  
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'en-US,en;q=0.9',
        'Cache-Control': 'max-age=0',
        'Pragma': 'no-cache',
        'Referrer': 'https://google.com',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
    }

    return requests.get(url, headers=headers)

#Ensure that some table rows are found
def parse_rows(table_rows):
    parsed_rows = []

    for table_row in table_rows:
        parsed_row = []
        el = table_row.xpath("./div")

        none_count = 0

        for rs in el:
            try:
                (text,) = rs.xpath('.//span/text()[1]')
                parsed_row.append(text)
            except ValueError:
                parsed_row.append(np.NaN)
                none_count += 1

        if (none_count < 4):
            parsed_rows.append(parsed_row)
            
    return pd.DataFrame(parsed_rows)

# Set the index to the first column: 'Period Ending' and transpose the DataFrame, so that our header contains the account names
def clean_data(df):
    df = df.set_index(0) 
    df = df.transpose() 
    
    # Rename the "Breakdown" column to "Date"
    cols = list(df.columns)
    cols[0] = 'Date'
    df = df.set_axis(cols, axis='columns', inplace=False)
    
    # We convert all columns into float numbers (numeric), except the first column which is the date
    numeric_columns = list(df.columns)[1::] 
    for column_index in range(1, len(df.columns)): 
        df.iloc[:,column_index] = df.iloc[:,column_index].str.replace(',', '') 
        df.iloc[:,column_index] = df.iloc[:,column_index].astype(np.float64) 
        
    return df

def scrape_table(url):
    # Fetch the page that we're going to parse
    page = get_page(url);

    # Parse the page with LXML, so that we can start doing some XPATH queries to extract the data that we want
    tree = html.fromstring(page.content)

    # Fetch all div elements which have class 'D(tbr)'
    table_rows = tree.xpath("//div[contains(@class, 'D(tbr)')]")
    
    # Ensure that some table rows are found
    assert len(table_rows) > 0
    
    df = parse_rows(table_rows)
    df = clean_data(df)
        
    return df

# We scrape data for the balance sheet, the income statement and the cash flow statement 
def scrape(symbol):
    print('Attempting to scrape data for ' + symbol)

    df_balance_sheet = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/balance-sheet?p=' + symbol)
    df_balance_sheet = df_balance_sheet.set_index('Date')

    df_income_statement = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/financials?p=' + symbol)
    df_income_statement = df_income_statement.set_index('Date')
    
    df_cash_flow = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/cash-flow?p=' + symbol)
    df_cash_flow = df_cash_flow.set_index('Date')
    
    # We create a joint table that will gather data from the balance sheet, the income statement and the statement of cash flows for one ticker in the same table
    df_joined = df_balance_sheet \
        .join(df_income_statement, on='Date', how='outer', rsuffix=' - Income Statement') \
        .join(df_cash_flow, on='Date', how='outer', rsuffix=' - Cash Flow') \
        .dropna(axis=1, how='all') \
        .reset_index()
    
    # We add a column that includes the name of the ticker to allow for a better understanding of the results
    df_joined.insert(1, 'Symbol', symbol) 
    
    return df_joined

# We create a new functions that will allow us to scrape data from the all the tickers list that the user inputed before, and run each of tehm through the scrape function we created before
def scrape_multi(tickers):
    return pd.concat([scrape(symbol) for symbol in tickers], sort=False)

# This is the table that will include gather information from the balance sheet, the income statement and the statement of cash flows for all the stocks selected by the user
df_combined = scrape_multi(tickers)

# We ask the program to display all the columns avialable and not hide some of them
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# We print the table using pandas to obtain a table layout
df_combined_pandas = pd.DataFrame(df_combined)

display(df_combined_panda)


# To allow for some comparison between the stocks, we normalize the adjusted closing prices and plot them 
normalized_price_historical = price_historical.pct_change().dropna()
(normalized_price_historical+1).cumprod().plot(figsize=(15,5))
plt.xlabel('Date')
plt.ylabel('Adjusted Closing Price')
plt.title('Evolution of the Adjusted Closing Price')

#Here we display some additional information about the stock historical closing prices
perc =[.20, .40, .60, .80]
print(price_historical.describe(percentiles = perc))



# To allow for some comparison between the stocks, we normalize the volumes traded and plot them 
normalized_volume_historical = volume_historical.pct_change().dropna()
(normalized_volume_historical+1).cumprod().plot(figsize=(15,5))
plt.xlabel('Date')
plt.ylabel('Adjusted Volume Traded')
plt.title('Evolution of the Adjusted Volume Traded')


##################################################################################


# Computing the variance-covariance matrix
cov_mat = log_ret.cov() * 252 #252 trading days in a year
print(cov_mat)

# number of portfolio to simulate
n_portfolio = 7500

# Creating empty arrays to store portfolio weights, return, risk, and sharpe ratio 
all_weights = np.zeros((n_portfolio, len(price_historical.columns)))
port_returns = np.zeros((n_portfolio))
port_risk = np.zeros((n_portfolio))
sharpe_ratio = np.zeros((n_portfolio))

# Simulating the 7500 portfolios and storing their weights, returns, volatility, sharpe ratio
for i in range(n_portfolio):
  # Generating randon weigths using a uniform distribution
  weights = np.random.uniform(size = len(price_historical.columns))
  weights = weights/np.sum(weights)
  
  # Storing weights 
  all_weights[i,:] = weights
  
  # Storing return
  port_ret = np.sum(log_ret.mean() * weights)
  port_ret = (port_ret + 1) ** 252 - 1
  port_returns[i] = port_ret
  
  # Storing risk
  port_sd = np.sqrt(np.dot(weights.T, np.dot(cov_mat, weights)))
  port_risk[i] = port_sd
  
  # Storing Sharpe Ratio  
  sr = (port_ret - riskfree) / port_sd
  sharpe_ratio[i] = sr

names = price_historical.columns

# Finding the minimum variance portfolio
min_var = all_weights[port_risk.argmin()]
print("The weights of the minimum variance portfolio are:", min_var)

# Finding the tangency portfolio
max_sr = all_weights[sharpe_ratio.argmax()]
print("The weights of the maximum sharpe ratio portfolio are:", max_sr)

# Finding the equally weighted portfolio
eq_weights = []
for i in range(len(weights)):
 eq_weights.append(1/len(weights))
print("The weights of the equally weigthed portfolio are:", eq_weights)

# Finding the maximum return portfolio
max_ret = all_weights[port_ret.argmax()]
print("The weights of the maximum return portfolio are:", max_ret)


# Check if portfolio weights sum to 1
print(sum(min_var), sum(max_sr), sum(eq_weights), sum(max_ret))



################################################################################

# Here we can probably add some graphs and performance and risk metrics for each optimization so that the user can select the optimization he prefers

################################################################################
